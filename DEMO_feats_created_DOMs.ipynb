{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pertubation of features-created domains"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from src.loader import load_graph\n",
    "from classes import extract_all_features_single \n",
    "import sys\n",
    "import os\n",
    "from src import utils\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch_geometric.transforms as T\n",
    "from src.utils import largest_indices\n",
    "from src.utils import cal_n_add_facni \n",
    "from src.utils import extract_feat_adj2\n",
    "from src.utils import calc_cad \n",
    "import networkx as nx\n",
    "from scipy import spatial\n",
    "import copy  \n",
    "from matplotlib import pyplot as plt \n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Routines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confid_measures(arr_tri, Num):\n",
    "    arr_tri_mean=np.mean(arr_tri,axis=0)\n",
    "    std=np.std(arr_tri,axis=0)\n",
    "    Z=1.960 # for 95 conf.\n",
    "    upper=arr_tri_mean+Z*std/np.sqrt(Num)\n",
    "    lower=arr_tri_mean-Z*std/np.sqrt(Num)\n",
    "    return arr_tri_mean, lower, upper\n",
    "\n",
    "def my_score(pred, labels):\n",
    "    accuracy = (pred == labels).sum() / len(pred)\n",
    "    return accuracy\n",
    "def my_own_acc(a, b):\n",
    "    acc = np.sum(np.equal(a, b)) / len(a)\n",
    "    return acc\n",
    "\n",
    "def do_perturb_feat(x,m):\n",
    "    x_bol=np.array(x, dtype=bool);\n",
    "    m_bol=np.array(m, dtype=bool);\n",
    "#     x2=np.logical_or(x_bol,m_bol)\n",
    "    x2=np.logical_xor(x_bol,m_bol)\n",
    "    x2=x2.astype(float)\n",
    "    x2=torch.from_numpy(x2)\n",
    "    return x2\n",
    "\n",
    "def do_perturb_adj(a,m):\n",
    "    a_bol=np.array(a, dtype=bool);\n",
    "    m_bol=np.array(m, dtype=bool);\n",
    "    a2=np.logical_xor(a_bol,m_bol)\n",
    "    a2=a2.astype(float)\n",
    "    a2=torch.from_numpy(a2)\n",
    "    return a2\n",
    "\n",
    "def A_to_edge_index(A):\n",
    "    G=nx.from_numpy_matrix(A)\n",
    "    edge_index=list(G.edges)\n",
    "    z=torch.tensor(np.transpose(edge_index))\n",
    "    return z\n",
    "\n",
    "def assign_Adversary_nas_mal(data, norm_zero_int):\n",
    "\n",
    "    test_mask = data['domain_node']['test_mask']\n",
    "    labels_test=data['domain_node'].y[test_mask].cpu()\n",
    "    lox_test=np.where(test_mask.cpu()>0)\n",
    "    lox_test=lox_test[0]\n",
    "    labels=data['domain_node'].y.cpu()\n",
    "\n",
    "    lox_test_space=lox_test[np.where(labels_test.cpu()==1) ] #mal\n",
    "\n",
    "    adv_nodes_test=random.sample(set(lox_test_space), norm_zero_int)\n",
    "    \n",
    "    return adv_nodes_test\n",
    "\n",
    "def get_As_new(adj_1, adj_2, adj_3, adj_4, adv_nodes):\n",
    "    \n",
    "    edge_list= np.concatenate(( adj_2.cpu(), adj_4.cpu()), axis=1)\n",
    "   \n",
    "    all_edges1=edge_list[0,:]\n",
    "    all_edges2=edge_list[1,:]\n",
    "\n",
    "    adv_edge_lox1= np.nonzero(np.in1d(all_edges1, adv_nodes))[0]\n",
    "    adv_edge_lox2= np.nonzero(np.in1d(all_edges2, adv_nodes))[0]\n",
    "    A_adv=np.zeros( (len(adv_nodes),len(adv_nodes)), dtype=int)\n",
    "    adv_nodes=np.array(adv_nodes)\n",
    "    adv_edge_lox1=np.array(adv_edge_lox1)\n",
    "    adv_edge_lox2=np.array(adv_edge_lox2)\n",
    "\n",
    "    for k in range(adv_edge_lox1.shape[0]):\n",
    "        a=all_edges1[adv_edge_lox1[k]]\n",
    "        b=all_edges2[adv_edge_lox2[k]]\n",
    "        lox_a=np.where(adv_nodes == np.array(a))\n",
    "        lox_b=np.where(adv_nodes == np.array(b))\n",
    "        A_adv[lox_a,lox_b]=1 \n",
    "    print(\"resssss\", np.count_nonzero(A_adv))\n",
    "   \n",
    "    return A_adv\n",
    "\n",
    "def extract_A(data, adv_nodes):\n",
    "    edge_list_1=data.edge_index_dict['domain_node', 'apex', 'domain_node'][0,:]\n",
    "    edge_list=data.edge_index_dict['domain_node', 'apex', 'domain_node']\n",
    "    common_node_lox=np.nonzero(np.in1d(edge_list_1.cpu(), adv_nodes))[0]\n",
    "    adv_edge_list=edge_list[:,common_node_lox]\n",
    "    A_adv=edge_list_to_adj(adv_edge_list.cpu())\n",
    "    return A_adv\n",
    "\n",
    "def edge_list_to_adj(adv_edge_lox1, adv_edge_lox2):\n",
    "    elist=adv_edge_lox2\n",
    "    print(elist)\n",
    "    domain_node_list=np.unique(elist)\n",
    "    domain_node_list=domain_node_list[0:4000]\n",
    "    A=np.zeros( (len(domain_node_list),len(domain_node_list)), dtype=int)\n",
    "    for k in range(len(elist)):\n",
    "        a=adv_edge_lox1[k]\n",
    "        b=adv_edge_lox2[k]\n",
    "        lox_a=np.where(domain_node_list == a)\n",
    "        lox_b=np.where(domain_node_list == b)\n",
    "        A[lox_a,lox_b]=1\n",
    "    return A\n",
    "\n",
    "def extract_As_jan(data, adv_nodes):\n",
    "    edge_list=data.edge_index_dict['domain_node', 'apex', 'domain_node']\n",
    "    all_edges1=edge_list[0,:]\n",
    "    all_edges2=edge_list[1,:] \n",
    "    all_edges1=all_edges1.cpu()\n",
    "    all_edges2=all_edges2.cpu()\n",
    "    adv_edge_lox1= np.nonzero(np.in1d(all_edges1, adv_nodes))[0]\n",
    "    adv_edge_lox2= np.nonzero(np.in1d(all_edges2, adv_nodes))[0]\n",
    "    L=len(adv_nodes)\n",
    "    A_adv1=np.zeros( (L,L), dtype=int)\n",
    "    adv_nodes=np.array(adv_nodes)\n",
    "    adv_edge_lox1=np.array(adv_edge_lox1)\n",
    "    adv_edge_lox2=np.array(adv_edge_lox2)\n",
    "\n",
    "    for k in range(adv_edge_lox1.shape[0]):\n",
    "        a=all_edges1[adv_edge_lox1[k]]\n",
    "        b=all_edges2[adv_edge_lox2[k]]\n",
    "        lox_a=np.where(adv_nodes == np.array(a))\n",
    "        lox_b=np.where(adv_nodes == np.array(b))\n",
    "        A_adv1[lox_a,lox_b]=1     \n",
    "    print(\"sparsity\", np.count_nonzero(A_adv1))    \n",
    "    \n",
    "    \n",
    "    edge_list=data.edge_index_dict['domain_node', 'similar', 'domain_node']\n",
    "    all_edges1=edge_list[0,:]\n",
    "    all_edges2=edge_list[1,:]\n",
    "    \n",
    "    all_edges1=all_edges1.cpu()\n",
    "    all_edges2=all_edges2.cpu()\n",
    "\n",
    "    adv_edge_lox1= np.nonzero(np.in1d(all_edges1, adv_nodes))[0]\n",
    "    adv_edge_lox2= np.nonzero(np.in1d(all_edges2, adv_nodes))[0]\n",
    "\n",
    "    L=len(adv_nodes)\n",
    "    A_adv2=np.zeros( (L,L), dtype=int)\n",
    "    adv_nodes=np.array(adv_nodes)\n",
    "    adv_edge_lox1=np.array(adv_edge_lox1)\n",
    "    adv_edge_lox2=np.array(adv_edge_lox2)\n",
    "\n",
    "    for k in range(adv_edge_lox1.shape[0]):\n",
    "        a=all_edges1[adv_edge_lox1[k]]\n",
    "        b=all_edges2[adv_edge_lox2[k]]\n",
    "        lox_a=np.where(adv_nodes == np.array(a))\n",
    "        lox_b=np.where(adv_nodes == np.array(b))\n",
    "        A_adv2[lox_a,lox_b]=1\n",
    "        \n",
    "    print(\"sparsity\", np.count_nonzero(A_adv2)) \n",
    "    \n",
    "    \n",
    "    A_adv=A_adv1+A_adv2\n",
    "    \n",
    "    return A_adv\n",
    "\n",
    "\n",
    "def find_my_soln(WW):\n",
    "    ATA=np.dot(WW,WW.T)\n",
    "    w, v=np.linalg.eig(ATA)\n",
    "    return v[:,0]\n",
    "\n",
    "def calc_ASR(data, adv_nodes_test, model0):\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        model0.eval()\n",
    "        pred_raw0 = model0(data.x_dict, data.edge_index_dict)\n",
    "        pred_raw0 = F.softmax(pred_raw0, dim=1)\n",
    "        y0_hat= pred_raw0.argmax(dim=-1)\n",
    "        \n",
    "    y0_hat=y0_hat[adv_nodes_test]    \n",
    "\n",
    "    with torch.no_grad():\n",
    "        model0.eval()\n",
    "        pred_raw2 = model0(data.x_dict2, data.edge_index_dict)\n",
    "        pred_raw2 = F.softmax(pred_raw2, dim=1)\n",
    "        y2_hat= pred_raw2.argmax(dim=-1)\n",
    "    y2_hat=y2_hat[adv_nodes_test]        \n",
    "\n",
    "    num_of_1=0;\n",
    "    num_of_1_forced_to_0=0;\n",
    "    num_of_0=0;\n",
    "    num_of_0_forced_to_1=0;\n",
    "    \n",
    "    for jj in range(len(y0_hat)):\n",
    "        if y0_hat[jj]==1:\n",
    "            num_of_1=num_of_1+1\n",
    "        if y0_hat[jj]==1 and y2_hat[jj]==0:\n",
    "            num_of_1_forced_to_0=num_of_1_forced_to_0+1;\n",
    "            \n",
    "        if y0_hat[jj]==0:\n",
    "            num_of_0=num_of_0+1\n",
    "        if y0_hat[jj]==0 and y2_hat[jj]==1:\n",
    "            num_of_0_forced_to_1=num_of_0_forced_to_1+1;\n",
    "            \n",
    "    if num_of_1>0:\n",
    "        ASRgood=num_of_1_forced_to_0/num_of_1\n",
    "    else:\n",
    "        ASRgood=num_of_1_forced_to_0\n",
    "\n",
    "        \n",
    "    if num_of_0>0:\n",
    "        ASRbad=num_of_0_forced_to_1/num_of_0\n",
    "    else:\n",
    "        ASRbad=num_of_0_forced_to_1\n",
    "        \n",
    "    print(\"ASRgood\", ASRgood, num_of_1_forced_to_0, num_of_1, \"ASRbad\", ASRbad, num_of_0_forced_to_1, num_of_0)\n",
    "    return ASRgood, num_of_1_forced_to_0, num_of_1, ASRbad, num_of_0_forced_to_1, num_of_0\n",
    "def flip_the_bins(x,lox):\n",
    "    m=np.zeros_like(x)\n",
    "    m[:,lox]=1\n",
    "    x_bol=np.array(x, dtype=bool);\n",
    "    m_bol=np.array(m, dtype=bool);\n",
    "    x2=np.logical_xor(x_bol,m_bol)\n",
    "    x2=x2.astype(float)\n",
    "    x2=torch.from_numpy(x2)\n",
    "    return x2\n",
    "\n",
    "def preds_of_adv(model0, data, adv_nodes_test):\n",
    "    with torch.no_grad():\n",
    "        model0.eval()\n",
    "    pred_raw0 = model0(data.x_dict, data.edge_index_dict)\n",
    "    y0_hat= pred_raw0.argmax(dim=-1)\n",
    "    preds=y0_hat[adv_nodes_test]\n",
    "    return preds\n",
    "def model_qurey(model, data, idx_train):\n",
    "    model.eval()\n",
    "    pred_raw2 = model(data.x_dict2, data.edge_index_dict)\n",
    "    pred_raw2 = F.softmax(pred_raw2, dim=1)\n",
    "    y2_hat= pred_raw2.argmax(dim=-1)\n",
    "    labels_sur=y2_hat[idx_train]\n",
    "    \n",
    "    return labels_sur"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from src.loader2 import DNS2\n",
    "kg_path = lambda graph_name: f'/home/lambda/Desktop/Mahmoud/MDD Adv attack feb 11/Experiments 0 Perf Eval/myGraph_datasets/{graph_name}'\n",
    "dataset = DNS2('myGraph_datasets/DNS_eid_adv', transform=T.Compose([T.NormalizeFeatures(), T.ToUndirected()]), balance_gt=True)\n",
    "data = dataset[0]\n",
    "# dir(data)\n",
    "# feats_2=cal_n_add_facni(kg_path('DNS_eid_adv'), data);\n",
    "# torch.save(feats_2, 'feats_2.pt')\n",
    "# This script is for eature extraction\n",
    "# import warnings\n",
    "# warnings.filterwarnings('ignore')\n",
    "# dir(data)\n",
    "# feats_new2=cal_n_add_facni(kg_path('DNS_eid_adv'), data);\n",
    "# torch.save(feats_new2, 'feats_new2.pt')\n",
    "\n",
    "# Feature assignment \n",
    "feats_new2=torch.load('feats_new2.pt')\n",
    "data['domain_node'].x= feats_new2[0:data.x_dict['domain_node'].shape[0],:]\n",
    "data['ip_node'].x=torch.zeros(data['ip_node'].x.shape[0],1)\n",
    "data['host_node'].x=torch.zeros(data['host_node'].x.shape[0],1)\n",
    "del feats_new2\n",
    "# print(data.metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The MDD model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from torch_geometric.nn import GATConv, HeteroConv, Linear\n",
    "\n",
    "class HeteroGNN(torch.nn.Module):\n",
    "    def __init__(self, metadata, hidden_channels, out_channels, num_layers):\n",
    "        super().__init__()\n",
    "\n",
    "        self.convs = torch.nn.ModuleList()\n",
    "        for _ in range(num_layers):\n",
    "            conv = HeteroConv({\n",
    "                edge_type: GATConv((-1,-1), hidden_channels)\n",
    "                for edge_type in metadata[1]\n",
    "            })\n",
    "            self.convs.append(conv)\n",
    "\n",
    "        self.lin = Linear(hidden_channels, out_channels)\n",
    "        \n",
    "    def forward(self, x_dict, edge_index_dict):\n",
    "        for conv in self.convs:\n",
    "            x_dict = conv(x_dict, edge_index_dict)\n",
    "            x_dict = {key: F.leaky_relu(x) for key, x in x_dict.items()}\n",
    "        return self.lin(x_dict['domain_node'])\n",
    "    \n",
    "model = HeteroGNN(data.metadata(), hidden_channels=64, out_channels=2,\n",
    "                  num_layers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.set_device(1)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "data=data.to(device)\n",
    "model=model.to(device)\n",
    "\n",
    "with torch.no_grad():  # Initialize lazy modules.\n",
    "    out = model(data.x_dict, data.edge_index_dict)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.005, weight_decay=0.001)\n",
    "           \n",
    "def train(model,data):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data.x_dict, data.edge_index_dict)\n",
    "    mask = data['domain_node'].train_mask\n",
    "    loss = F.cross_entropy(out[mask], data['domain_node'].y[mask])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return float(loss)\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(model,data):\n",
    "    model.eval()\n",
    "    pred = model(data.x_dict, data.edge_index_dict).argmax(dim=-1)\n",
    "    accs = []\n",
    "    for split in ['train_mask', 'val_mask', 'test_mask']:\n",
    "        mask = data[split]\n",
    "        acc = (pred[mask] == data['domain_node'].y[mask]).sum() / mask.sum()\n",
    "        accs.append(float(acc))\n",
    "    return accs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Surrogate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deeprobust.graph.defense import GCN\n",
    "# datya to train the surrog\n",
    "train_mask = data['domain_node'].train_mask\n",
    "labels_train=data['domain_node'].y.cpu()\n",
    "labelled_labels_train_lox=np.where(labels_train<2)\n",
    "lox_train=np.where(train_mask.cpu()>0)\n",
    "lox_train=lox_train[0]\n",
    "labels=data['domain_node'].y.cpu()\n",
    "lox_train_space=labelled_labels_train_lox\n",
    "adv_nodes_train=lox_train_space[0][0:4000]\n",
    "Adj_sur=extract_As_jan(data, adv_nodes_train)\n",
    "features_sur=data['domain_node'].x[adv_nodes_train]\n",
    "features_sur=np.array(features_sur.cpu())\n",
    "data.x_sur=features_sur\n",
    "idx_train=np.arange(4000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The proposed MinstA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_trial(adv_nodes_test):\n",
    "    \n",
    "    # init and a train a new model instance\n",
    "    dataset = DNS2('myGraph_datasets/DNS_eid_adv', transform=T.Compose([T.NormalizeFeatures(), T.ToUndirected()]), balance_gt=True)\n",
    "    data = dataset[0]\n",
    "    feats_new=torch.load('feats_new2.pt')\n",
    "    data['domain_node'].x= feats_new[0:data.x_dict['domain_node'].shape[0],:]\n",
    "    data['ip_node'].x=torch.zeros(data['ip_node'].x.shape[0],1)\n",
    "    data['host_node'].x=torch.zeros(data['host_node'].x.shape[0],1)\n",
    "    del feats_new\n",
    "        \n",
    "    perf_arr=np.empty((6,0))\n",
    "    val_nodes=np.array([1,5,20,40,60,80,100])\n",
    "    \n",
    "        # init and train a new clean model:\n",
    "    model0= HeteroGNN(data.metadata(), hidden_channels=64, out_channels=2, num_layers=2)\n",
    "    data, model0 = data.to(device), model0.to(device)\n",
    "    data.x_dict2=copy.deepcopy(data.x_dict)\n",
    "    \n",
    "    for epoch in range(201):\n",
    "        loss=train(model0,data)\n",
    "        \n",
    "    labels_sur=model_qurey(model0, data, idx_train)\n",
    "    labels_sur=np.array(labels_sur.cpu())\n",
    "    \n",
    "    preds=preds_of_adv(model0, data, adv_nodes_test)\n",
    "\n",
    "    # init and train a new surrogate\n",
    "    surrogate = GCN(nfeat=features_sur.shape[1], nclass=2,\n",
    "                nhid=64, dropout=0, with_relu=False, with_bias=False, device='cpu')\n",
    "    surrogate.fit(features_sur, Adj_sur, labels_sur, idx_train)\n",
    "\n",
    "    \n",
    "\n",
    "    A_adv=np.eye(100)\n",
    "    arr1=np.arange(1, 4+1)\n",
    "    arr2=np.arange(10, 18+1)\n",
    "    arr3=np.arange(20, 28+1)\n",
    "    arr4=np.arange(29, 46+1)\n",
    "    arr5=np.arange(47, 54+1)\n",
    "    arr6=np.arange(62, 66+1)\n",
    "    arr7=np.arange(67, 93+1)\n",
    "    arr8=np.arange(94, 99+1)\n",
    "\n",
    "    for i in range(len(arr1)):\n",
    "        for j in range(len(arr1)):\n",
    "            A_adv[arr1[i], arr1[j]]=1       \n",
    "    for i in range(len(arr2)):\n",
    "        for j in range(len(arr2)):\n",
    "            A_adv[arr2[i], arr2[j]]=1        \n",
    "    for i in range(len(arr3)):\n",
    "        for j in range(len(arr3)):\n",
    "            A_adv[arr3[i], arr3[j]]=1              \n",
    "    for i in range(len(arr4)):\n",
    "        for j in range(len(arr4)):\n",
    "            A_adv[arr4[i], arr4[j]]=1\n",
    "\n",
    "    for i in range(len(arr5)):\n",
    "        for j in range(len(arr5)):\n",
    "            A_adv[arr5[i], arr5[j]]=1       \n",
    "    for i in range(len(arr6)):\n",
    "        for j in range(len(arr6)):\n",
    "            A_adv[arr6[i], arr6[j]]=1        \n",
    "    for i in range(len(arr7)):\n",
    "        for j in range(len(arr7)):\n",
    "            A_adv[arr7[i], arr7[j]]=1              \n",
    "    for i in range(len(arr8)):\n",
    "        for j in range(len(arr8)):\n",
    "            A_adv[arr8[i], arr8[j]]=1\n",
    "   \n",
    "    for val in val_nodes: \n",
    "        print(val)\n",
    "        x=data['domain_node'].x.cpu()\n",
    "        x2=copy.deepcopy(x)\n",
    "        X=x[adv_nodes_test,:].cpu()\n",
    "        X=np.array(X)\n",
    "        A=A_adv\n",
    "        X2=copy.deepcopy(X)\n",
    "        all_lox=np.arange(45)\n",
    "        w1=surrogate.gc1\n",
    "        W=w1.weight.data\n",
    "        W1=np.array(W.cpu())\n",
    "        F1=find_my_soln(W1)\n",
    "\n",
    "        for zzz in range(1):\n",
    "            for i in np.arange(val):\n",
    "                if preds[i]>0:\n",
    "                    temp=A[i,:];\n",
    "                    js=np.flatnonzero(temp) \n",
    "                #     print(\"js\", js)\n",
    "                    messages=np.dot(A, X2)\n",
    "                    F2=0*F1\n",
    "                    for kk in np.arange(len(js)):\n",
    "                        message_j=messages[js[kk],:]\n",
    "\n",
    "                        W2=copy.deepcopy(W1)\n",
    "                        d_j=len(js)\n",
    "                #         F2=copy.deepcopy(F1)\n",
    "                        for kkk in np.arange(64):\n",
    "                            if d_j>0:\n",
    "                                W2[:,kkk]=W1[:,kkk]-1/d_j*message_j\n",
    "                            F2=F2+find_my_soln(W2)    \n",
    "                    feat_up=(1*F1+1*F2)/2\n",
    "                    X2[i,:]=X2[i,:]+feat_up[:]   \n",
    "\n",
    "        x2[adv_nodes_test,:]=torch.from_numpy(X2)  \n",
    "        x2=x2.to(device)\n",
    "        data.x_dict2['domain_node']=x2\n",
    "        ASRgood, num_of_1_forced_to_0, num_of_1, ASRbad, num_of_0_forced_to_1, num_of_0=calc_ASR(data, adv_nodes_test, model0) \n",
    "\n",
    "        temp=np.array([ASRgood, num_of_1_forced_to_0, num_of_1, ASRbad, num_of_0_forced_to_1, num_of_0]).reshape(-1,1)\n",
    "        perf_arr = np.hstack((perf_arr, temp))\n",
    "        \n",
    "    return perf_arr    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Num=30\n",
    "res_arr=np.zeros((Num, 6,7))\n",
    "for tri in range(Num):    \n",
    "    adv_nodes=np.arange(100)+377654-100\n",
    "    g1=one_trial(adv_nodes)\n",
    "    res_arr[tri,:,:]=g1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr1 = np.mean(res_arr, axis = 0)\n",
    "avg_mal_num=res_arr[:,2,:].mean()\n",
    "avg_ben_num=res_arr[:,5,:].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### print(arr1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_nodes=np.array([1,5,20,40,60,80,100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['font.size'] = 17\n",
    "plt.rcParams['font.weight'] = 'bold'\n",
    "plt.rcParams['axes.titleweight'] = 'bold'\n",
    "plt.rcParams['axes.labelweight'] = 'bold'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot ASR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_change_arr=val_nodes\n",
    "\n",
    "if not os.path.exists('RESULTS_feat_creatd_doms'):\n",
    "    os.makedirs('RESULTS_feat_creatd_doms')\n",
    "plt.figure(facecolor='white')\n",
    "plt.title(\"Feature Perturbation\")\n",
    "plt.xlabel(\"Domain nodes with perturbed edges\", weight='bold') \n",
    "plt.ylabel(\"ASR\", weight='bold') \n",
    "# plot the ASR:\n",
    "m0, l0, u0=confid_measures(res_arr_nz3[:,0,:], Num)\n",
    "plt.plot(val_nodes, m0,'-og', label=('ASR'))\n",
    "plt.fill_between(num_change_arr, l0, u0, color='green', alpha=0.2)\n",
    "plt.grid()\n",
    "legend_text = 'Mal. doms: %.1f' % (arr1[2,0])\n",
    "plt.legend(loc='lower right',prop=dict(weight='bold'),title=legend_text,  fontsize=\"20\")\n",
    "name='RESULTS_feat_creatd_doms/asr_plot1_'+ str(Num)+'_trials.png'\n",
    "plt.savefig(name, dpi=150, bbox_inches = 'tight') \n",
    "name='RESULTS_feat_creatd_doms/asr_plot1_'+ str(Num)+'_trials.pdf'\n",
    "plt.savefig(name, dpi=150, bbox_inches = 'tight') \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot NFR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_change_arr=val_nodes\n",
    "# Create a figure and axis\n",
    "fig, ax = plt.subplots()\n",
    "ax.yaxis.set_major_formatter(plt.FormatStrFormatter('%.2f'))\n",
    "plt.title(\"Feature Perturbation\")\n",
    "plt.xlabel(\"Domain nodes with perturbed edges\", weight='bold') \n",
    "plt.ylabel(\"NFR\", weight='bold') \n",
    "m3, l3, u3=confid_measures(res_arr_nz3[:,3,:], Num)\n",
    "ax.plot(val_nodes, m3,'-xr', label=(\"NFR\"))\n",
    "ax.fill_between(num_change_arr, l3, u3, color='red', alpha=0.2)\n",
    "ax.grid()\n",
    "legend_text = 'Undetected doms: %.1f' % (arr1[5,0])\n",
    "plt.legend(loc='upper left', fontsize=\"11\",prop=dict(weight='bold'),title=legend_text)\n",
    "name='RESULTS_feat_creatd_doms/asr_plot3_'+ str(Num)+'_trials_.png'\n",
    "plt.savefig(name, dpi=150, bbox_inches = 'tight') \n",
    "name='RESULTS_feat_creatd_doms/asr_plot3_'+ str(Num)+'_trials_.pdf'\n",
    "plt.savefig(name, dpi=150, bbox_inches = 'tight') \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot ROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tpr=m0\n",
    "fpr=m3\n",
    "plt.figure(facecolor='white')\n",
    "plt.title(\"NFR-ASR ROC\")\n",
    "plt.xlabel(\"NFR\", weight='bold') \n",
    "plt.ylabel(\"ASR\", weight='bold') \n",
    "m3, l3, u3=confid_measures(res_arr_nz3[:,3,:], 100)\n",
    "plt.plot(fpr, tpr) \n",
    "plt.grid()\n",
    "name='RESULTS_feat_creatd_doms/feat_ROC_mydoms'+ str(Num)+'.png'\n",
    "plt.savefig(name, dpi=150, bbox_inches = 'tight') \n",
    "name='RESULTS_feat_creatd_doms/feat_ROC_mydoms'+ str(Num)+'.pdf'\n",
    "plt.savefig(name, dpi=150, bbox_inches = 'tight') \n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
